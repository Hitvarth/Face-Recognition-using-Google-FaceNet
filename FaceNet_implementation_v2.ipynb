{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FaceNet implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB9EuqrwGxXo",
        "outputId": "73563c27-9e94-4896-e0fb-57dbf55fec0f"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBDxTlAder2_",
        "outputId": "a1740196-ab03-4ed7-d1ef-89e8fad60e70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g0so64Ab-MJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np \n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txJpveILitgN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e567c46-ed34-4ce1-d8fd-5a1e426f733f"
      },
      "source": [
        "data_train = list()\n",
        "data_val   = list()\n",
        "data_test  = list()\n",
        "\n",
        "for subject in os.listdir('gdrive/MyDrive/Database 2'):\n",
        "  for subfolder in os.listdir(os.path.join('gdrive/MyDrive/Database 2/',subject)):\n",
        "    path = os.path.join('gdrive/MyDrive/Database 2/',subject, subfolder)\n",
        "    a = 0\n",
        "    for image in os.listdir(path):\n",
        "      img = cv2.imread(os.path.join(path,image),cv2.IMREAD_COLOR)\n",
        "      img = cv2.resize(img,(220,220))\n",
        "      ## train:70, val:20, test:10\n",
        "      if a%10<=6:\n",
        "        data_train.append([img,subject])\n",
        "      elif a%10<=8:\n",
        "        data_val.append([img,subject])\n",
        "      elif a%10==9:\n",
        "        data_test.append([img,subject])\n",
        "        \n",
        "      a+=1\n",
        "\n",
        "np_data_train = np.array(data_train)\n",
        "np_data_val = np.array(data_val)\n",
        "np_data_test = np.array(data_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VIh5x6JlGPlx",
        "outputId": "b0522009-5f24-4299-c94f-6c42d84967be"
      },
      "source": [
        "np_data_train[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Aaron_Eckhart'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkHkkclIEfHG"
      },
      "source": [
        "## shuffling the dataset\n",
        "np.random.shuffle(np_data_train)\n",
        "np.random.shuffle(np_data_val)\n",
        "np.random.shuffle(np_data_test)\n",
        "\n",
        "# now the data is shuffled\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for img, subject in np_data_train:\n",
        "  x_train.append(img)\n",
        "  y_train.append(subject)\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "np.savez('x_train.npz',x_train)\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "for img, subject in np_data_val:\n",
        "  x_val.append(img)\n",
        "  y_val.append(subject)\n",
        "x_val = np.array(x_val)\n",
        "y_val = np.array(y_val)\n",
        "np.savez('x_val.npz',x_val)\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "for img, subject in np_data_test:\n",
        "  x_test.append(img)\n",
        "  y_test.append(subject)\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "np.savez('x_test.npz',x_test)\n",
        "\n",
        "\n",
        "# x_train, y_train = np.split(np_data_train,2,axis=1)\n",
        "# x_val, y_val     = np.split(np_data_val,2,axis=1)\n",
        "# x_test, y_test = np.split(np_data_test,2,axis=1)\n",
        "\n",
        "# x_train = np.array(np_data_train[i][0] for i in range(np_data_train.shape[0]))\n",
        "# x_val   = np.array(np_data_val[i][0] for i in range(np_data_val.shape[0]))\n",
        "# x_test  = np.array(np_data_test[i][0] for i in range(np_data_test.shape[0]))\n",
        "\n",
        "# y_train = np.array(np_data_train[i][1] for i in range(np_data_train.shape[0]))\n",
        "# y_val   = np.array(np_data_val[i][1] for i in range(np_data_val.shape[0]))\n",
        "# y_test  = np.array(np_data_test[i][1] for i in range(np_data_test.shape[0]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NZ5q2b9g90N",
        "outputId": "482d66fc-a584-462f-ca45-7476cae7d177"
      },
      "source": [
        "print(img.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(220, 220, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ouChsxRhKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bbf7f6-b02a-49ab-bbc5-2a057c5ccd47"
      },
      "source": [
        "# print(y_train[0])\n",
        "# print(x_train[0])\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(748, 220, 220, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRy6LWn7Vyti",
        "outputId": "e9668d89-97e9-4887-a61e-021eedfa4dad"
      },
      "source": [
        "## Encoding the subjects to integer labels\n",
        "\n",
        "# people_mapping = dict()\n",
        "# index = 0\n",
        "# for person in y_train:\n",
        "#   if person not in people_mapping.keys() :\n",
        "#     people_mapping[person]=index\n",
        "#     index+=1\n",
        "people_mapping = {'Aaron_Eckhart': 0,\n",
        "                   'Aaron_Guiel': 3,\n",
        "                   'Aaron_Sorkin': 1,\n",
        "                   'Aaron_Tippin': 4,\n",
        "                   'Abba_Eban': 2\n",
        "                  }\n",
        "\n",
        "y_train_int = np.array([people_mapping[value] for value in y_train])\n",
        "y_val_int   = np.array([people_mapping[value] for value in y_val])\n",
        "y_test_int  = np.array([people_mapping[value] for value in y_test])\n",
        "\n",
        "np.savez('y_train_int.npz',y_train_int)\n",
        "np.savez('y_val_int.npz',y_val_int)\n",
        "np.savez('y_test_int.npz',y_test_int)\n",
        "\n",
        "\n",
        "# y_train_int\n",
        "# y_val_int\n",
        "y_test_int"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 1, 0, 0, 3, 2, 0, 4, 4, 2, 2, 2, 4, 2, 3, 4, 4, 3, 0, 1, 1,\n",
              "       1, 4, 0, 1, 1, 1, 0, 4, 0, 2, 0, 1, 0, 2, 2, 1, 1, 1, 2, 1, 0, 4,\n",
              "       4, 0, 4, 2, 1, 4, 3, 3, 4, 0, 2, 2, 1, 4, 3, 0, 3, 1, 1, 2, 0, 1,\n",
              "       2, 1, 0, 3, 4, 3, 4, 4, 1, 4, 4, 2, 1, 4, 3, 2, 0, 2, 2, 3, 2, 3,\n",
              "       2, 0, 1, 2, 2, 2, 2, 1, 2, 2, 3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VoYqiV4ACjs",
        "outputId": "56831b2b-4b55-4839-9722-390f9cdad575"
      },
      "source": [
        "people_mapping"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Aaron_Eckhart': 0,\n",
              " 'Aaron_Guiel': 3,\n",
              " 'Aaron_Sorkin': 1,\n",
              " 'Aaron_Tippin': 4,\n",
              " 'Abba_Eban': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WmGGrthPtQf"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VReppRhGsMqI"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                     \n",
        "  tf.keras.layers.Conv2D(input_shape=(220,220,3), filters=64, kernel_size=(7,7), strides=(2,2), padding='same',activation='relu'),   # conv1\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),             # pool1\n",
        "  tf.keras.layers.BatchNormalization(), # not sure about this                                 # rnorm1\n",
        "  tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),  # conv2a\n",
        "  tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'), # conv2\n",
        "  tf.keras.layers.BatchNormalization(), # not sure about this                                 # rnorm2\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),             # pool2\n",
        "  tf.keras.layers.Conv2D(filters=192, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv3a\n",
        "  tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv3\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),            # pool3\n",
        "  tf.keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv4a\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv4\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv5a\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv5\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv6a\n",
        "  tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv6\n",
        "  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),              # pool4\n",
        "  # tf.keras.layers.Concatenate()\n",
        "  # not sure about the last 4 layers... \n",
        "  # tfa.layers.Maxout(num_units=2),  \n",
        "  # tfa.layers.Maxout(num_units=2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128,activation=None),\n",
        "  tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n",
        "\n",
        "])\n",
        "\n",
        "# seriously doubt the last four layers...\n",
        "# need to confirm the kernel of pool layers\n",
        "# need to add activations to all the layers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1VV23MasbT2"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tfa.losses.TripletSemiHardLoss(), metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrTUeArxd2wC",
        "outputId": "fe6c4f75-c072-4f0a-dd27-8643974b04de"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_22 (Conv2D)           (None, 110, 110, 64)      9472      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 55, 55, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 55, 55, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 55, 55, 64)        4160      \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 55, 55, 192)       110784    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 55, 55, 192)       768       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 28, 28, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 28, 28, 192)       37056     \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 28, 28, 192)       331968    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 14, 14, 192)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 14, 14, 384)       74112     \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 14, 14, 256)       884992    \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 14, 14, 256)       65792     \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 14, 14, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 14, 14, 256)       65792     \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 14, 14, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               1605760   \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 128)               0         \n",
            "=================================================================\n",
            "Total params: 4,371,072\n",
            "Trainable params: 4,370,560\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU8_7IGCPwJn"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z1TSgDvGoKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90d7431-732c-4938-dfed-4fdc079ef2bb"
      },
      "source": [
        "model.fit(\n",
        "    x = x_train,\n",
        "    y = y_train_int,\n",
        "    batch_size = 10,\n",
        "    epochs = 5,\n",
        "    verbose = 1,\n",
        "    validation_data = (x_val, y_val_int),\n",
        "    shuffle = True, # doesn't matter if we have only one epoch or no batches,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "75/75 [==============================] - 146s 2s/step - loss: 0.6671 - accuracy: 0.0394 - val_loss: 0.6908 - val_accuracy: 0.0291\n",
            "Epoch 2/5\n",
            "75/75 [==============================] - 143s 2s/step - loss: 0.3772 - accuracy: 0.0045 - val_loss: 0.5787 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/5\n",
            "75/75 [==============================] - 144s 2s/step - loss: 0.2000 - accuracy: 0.0000e+00 - val_loss: 0.4579 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/5\n",
            "75/75 [==============================] - 143s 2s/step - loss: 0.2028 - accuracy: 0.0000e+00 - val_loss: 0.1895 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/5\n",
            "75/75 [==============================] - 143s 2s/step - loss: 0.1133 - accuracy: 0.0000e+00 - val_loss: 0.4456 - val_accuracy: 0.0049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f36de84e810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZV0CP1n-dfU",
        "outputId": "57f3e5e1-c163-4a9b-950e-993d765d6fef"
      },
      "source": [
        "model.save('saved_model/embeddings_nn')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/embeddings_nn/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7LXWxuA_TQz"
      },
      "source": [
        "!pip install -q pyyaml h5py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKItthTDBLaU"
      },
      "source": [
        "## saving weights\n",
        "model.save_weights('embeddings_nn_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1dxN-PnPkT8"
      },
      "source": [
        "## Run the code from here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo4YEsqnP15r"
      },
      "source": [
        "!pip install -q pyyaml h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMUXdTXRBPR4"
      },
      "source": [
        "## loading weights\n",
        "model.load_weights('embeddings_nn_weights.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7I27Tfia_1a"
      },
      "source": [
        "with np.load('x_test.npz') as data:\n",
        "  for key in data:\n",
        "    x_test = data[key]\n",
        "  results = model.predict(x_test,batch_size=1)\n",
        "\n",
        "with np.load('x_train.npz') as data:\n",
        "  for key in data:\n",
        "    x_train = data[key]\n",
        "  results = np.append(results, model.predict(x_train,batch_size=1),axis=0)\n",
        "\n",
        "with np.load('x_val.npz') as data:\n",
        "  for key in data:\n",
        "    x_val = data[key]\n",
        "  results = np.append(results, model.predict(x_val,batch_size=1),axis=0)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LkGVI-Au0Jj",
        "outputId": "4575fc41-4e75-4724-d97c-9fa0232289f7"
      },
      "source": [
        "results.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1054, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HCi92MPvBxH",
        "outputId": "90fc4579-13d9-42e4-a16a-b5ba92071277"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 220, 220, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "H3UaTB8XvFch",
        "outputId": "57c7f160-0fb1-4aca-b93c-9a1efe335eff"
      },
      "source": [
        "# Save test embeddings for visualization in projector\n",
        "import io\n",
        "# import tensorflow_datasets as tfds\n",
        "np.savetxt(\"vecs.tsv\", results, delimiter='\\t')\n",
        "\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for img, label in np_data_test:\n",
        "    [out_m.write(label + \"\\n\")]\n",
        "for img, label in np_data_train:\n",
        "    [out_m.write(label + \"\\n\")]\n",
        "for img, label in np_data_val:\n",
        "    [out_m.write(label + \"\\n\")]\n",
        "out_m.close()\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cfc85525-2516-4a01-bd53-17d41293175f\", \"vecs.tsv\", 3442826)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_fdbf0d95-6fc8-4b48-a265-a3e5d10f1f61\", \"meta.tsv\", 12902)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP7YV0Zfz2LE"
      },
      "source": [
        "\"\"\"for img, labels in np_data_test:\n",
        "    # [out_m.write(str(x) + \"\\n\") for x in labels]\n",
        "    print([x for x in labels])\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VattDiJ2Y1H"
      },
      "source": [
        "#model which we can use for the multiclass classification with 5 people\n",
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer(input_shape=(128,), name ='layer1'),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\", name=\"layer2\"),\n",
        "        tf.keras.layers.Dense(1, activation='softmax', name=\"layer3\"),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTwmJ73E3R5t"
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCfKo2ao3XC6"
      },
      "source": [
        "model.fit(\n",
        "    x = x_train_2,\n",
        "    y = y_train_2,\n",
        "    batch_size = 1000,\n",
        "    epochs = 20,\n",
        "    verbose = 1,\n",
        "    validation_data = (x_val_2, y_val_2),\n",
        "    shuffle = True, # doesn't matter if we have only one epoch or no batches,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOtDF_a23aSJ"
      },
      "source": [
        "person = model.predict(x_test_2,batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}