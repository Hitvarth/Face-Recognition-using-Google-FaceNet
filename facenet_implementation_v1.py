# -*- coding: utf-8 -*-
"""FaceNet implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G2H6LmRAWMlp3jq2DFAl5adXEsRGwcir
"""

!pip install tensorflow-addons

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np 
import os
import pandas as pd
import cv2

data_train = list()
data_val   = list()
data_test  = list()

for subject in os.listdir('gdrive/MyDrive/Database 2'):
  for subfolder in os.listdir(os.path.join('gdrive/MyDrive/Database 2/',subject)):
    path = os.path.join('gdrive/MyDrive/Database 2/',subject, subfolder)
    a = 0
    for image in os.listdir(path):
      img = cv2.imread(os.path.join(path,image),cv2.IMREAD_COLOR)
      img = cv2.resize(img,(220,220))
      ## train:70, val:20, test:10
      if a%10<=6:
        data_train.append([img,subject])
      elif a%10<=8:
        data_val.append([img,subject])
      elif a%10==9:
        data_test.append([img,subject])
        
      a+=1

np_data_train = np.array(data_train)
np_data_val = np.array(data_val)
np_data_test = np.array(data_test)

np_data_train[0][1]

# testing random shuffle
b=[[1,2],[3,4],[5,6],[7,8]]
b=np.array(b)
np.random.shuffle(b)
b

## shuffling the dataset
np.random.shuffle(np_data_train)
np.random.shuffle(np_data_val)
np.random.shuffle(np_data_test)

# now the data is shuffled

x_train = []
y_train = []
for img, subject in np_data_train:
  x_train.append(img)
  y_train.append(subject)
x_train = np.array(x_train)
y_train = np.array(y_train)

x_val = []
y_val = []
for img, subject in np_data_val:
  x_val.append(img)
  y_val.append(subject)
x_val = np.array(x_val)
y_val = np.array(y_val)

x_test = []
y_test = []
for img, subject in np_data_test:
  x_test.append(img)
  y_test.append(subject)
x_test = np.array(x_test)
y_test = np.array(y_test)

# x_train, y_train = np.split(np_data_train,2,axis=1)
# x_val, y_val     = np.split(np_data_val,2,axis=1)
# x_test, y_test = np.split(np_data_test,2,axis=1)

# x_train = np.array(np_data_train[i][0] for i in range(np_data_train.shape[0]))
# x_val   = np.array(np_data_val[i][0] for i in range(np_data_val.shape[0]))
# x_test  = np.array(np_data_test[i][0] for i in range(np_data_test.shape[0]))

# y_train = np.array(np_data_train[i][1] for i in range(np_data_train.shape[0]))
# y_val   = np.array(np_data_val[i][1] for i in range(np_data_val.shape[0]))
# y_test  = np.array(np_data_test[i][1] for i in range(np_data_test.shape[0]))

print(img.shape)

# print(y_train[0])
# print(x_train[0])
x_train.shape

## Encoding the subjects to integer labels

people_mapping = dict()
index = 0
for person in y_train:
  if person not in people_mapping.keys() :
    people_mapping[person]=index
    index+=1

y_train_int = np.array([people_mapping[value] for value in y_train])
y_val_int   = np.array([people_mapping[value] for value in y_val])
y_test_int  = np.array([people_mapping[value] for value in y_test])

# y_train_int
# y_val_int
y_test_int

people_mapping

# from google.colab.patches import cv2_imshow
# # cv2_imshow(data[3000][0])
# data[500][0].shape

# model = tf.keras.models.Sequential([
                                     
#   tf.keras.layers.Conv3D(input_shape=(1,220,220,3), filters=64, kernel_size=(7,7,3), strides=(2,2,0), padding='same'),   # conv1
#   tf.keras.layers.MaxPool3D(pool_size=(3,3,1), strides=(2,2,1), padding='same'),             # pool1
#   tf.keras.layers.BatchNormalization(), # not sure about this                                 # rnorm1
#   tf.keras.layers.Conv3D(filters=64, kernel_size=(1,1,64), strides=(1,1,0), padding='same'),  # conv2a
#   tf.keras.layers.Conv3D(filters=192, kernel_size=(3,3,64), strides=(1,1,0), padding='same'), # conv2
#   tf.keras.layers.BatchNormalization(), # not sure about this                                 # rnorm2
#   tf.keras.layers.MaxPool3D(pool_size=(3,3,1), strides=(2,2,1), padding='same'),             # pool2
#   tf.keras.layers.Conv3D(filters=192, kernel_size=(1,1,192), strides=(1,1,0), padding='same'),# conv3a
#   tf.keras.layers.Conv3D(filters=192, kernel_size=(3,3,192), strides=(1,1,0), padding='same'),# conv3
#   tf.keras.layers.MaxPool3D(pool_size=(3,3,1), strides=(2,2,1), padding='same'),            # pool3
#   tf.keras.layers.Conv3D(filters=384, kernel_size=(1,1,384), strides=(1,1,0), padding='same'),# conv4a
#   tf.keras.layers.Conv3D(filters=256, kernel_size=(3,3,384), strides=(1,1,0), padding='same'),# conv4
#   tf.keras.layers.Conv3D(filters=256, kernel_size=(1,1,256), strides=(1,1,0), padding='same'),# conv5a
#   tf.keras.layers.Conv3D(filters=256, kernel_size=(3,3,256), strides=(1,1,0), padding='same'),# conv5
#   tf.keras.layers.Conv3D(filters=256, kernel_size=(1,1,256), strides=(1,1,0), padding='same'),# conv6a
#   tf.keras.layers.Conv3D(filters=256, kernel_size=(3,3,256), strides=(1,1,0), padding='same'),# conv6
#   tf.keras.layers.MaxPool3D(pool_size=(3,3,1), strides=(2,2,1), padding='same'),            # pool4
#   # tf.keras.layers.Concatenate()
#   # not sure about the last 4 layers... 
#   tfa.layers.Maxout(num_units=2),  
#   tfa.layers.Maxout(num_units=2),
#   tf.keras.layers.Flatten(),
#   tf.keras.layers.Dense(5,activation='softmax'),
# ])

# # seriously doubt the last four layers...
# # need to confirm the kernel of pool layers

model = tf.keras.models.Sequential([
                                     
  tf.keras.layers.Conv2D(input_shape=(220,220,3), filters=64, kernel_size=(7,7), strides=(2,2), padding='same',activation='relu'),   # conv1
  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),             # pool1
  tf.keras.layers.BatchNormalization(), # not sure about this                                 # rnorm1
  tf.keras.layers.Conv2D(filters=64, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),  # conv2a
  tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'), # conv2
  tf.keras.layers.BatchNormalization(), # not sure about this                                 # rnorm2
  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),             # pool2
  tf.keras.layers.Conv2D(filters=192, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv3a
  tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv3
  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),            # pool3
  tf.keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv4a
  tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv4
  tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv5a
  tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv5
  tf.keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), padding='same',activation='relu'),# conv6a
  tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same',activation='relu'),# conv6
  tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same'),              # pool4
  # tf.keras.layers.Concatenate()
  # not sure about the last 4 layers... 
  # tfa.layers.Maxout(num_units=2),  
  # tfa.layers.Maxout(num_units=2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128,activation=None),
  tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings

])

# seriously doubt the last four layers...
# need to confirm the kernel of pool layers
# need to add activations to all the layers

model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=tfa.losses.TripletSemiHardLoss(), metrics=['accuracy'])

model.summary()

model.fit(
    x = x_train,
    y = y_train_int,
    batch_size = 10,
    epochs = 5,
    verbose = 1,
    validation_data = (x_val, y_val_int),
    shuffle = True, # doesn't matter if we have only one epoch or no batches,
)

model.save('saved_model/embeddings_nn')

!pip install -q pyyaml h5py

model.save_weights('embeddings_nn_weights.h5')

results = model.predict(x_test,batch_size=1)

results.shape

x_test.shape

# Save test embeddings for visualization in projector
import io
# import tensorflow_datasets as tfds
np.savetxt("vecs.tsv", results, delimiter='\t')

out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for img, label in np_data_test:
    [out_m.write(label + "\n")]
out_m.close()

try:
  from google.colab import files
  files.download('vecs.tsv')
  files.download('meta.tsv')
except:
  pass

"""for img, labels in np_data_test:
    # [out_m.write(str(x) + "\n") for x in labels]
    print([x for x in labels])"""

#model which we can use for the multiclass classification with 5 people
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.InputLayer(input_shape=(128,), name ='layer1'),
        tf.keras.layers.Dense(64, activation="relu", name="layer2"),
        tf.keras.layers.Dense(1, activation='softmax', name="layer3"),
    ]
)

model.compile(optimizer='adam', 
              loss=tf.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(
    x = x_train_2,
    y = y_train_2,
    batch_size = 1000,
    epochs = 20,
    verbose = 1,
    validation_data = (x_val_2, y_val_2),
    shuffle = True, # doesn't matter if we have only one epoch or no batches,
)

person = model.predict(x_test_2,batch_size=1)